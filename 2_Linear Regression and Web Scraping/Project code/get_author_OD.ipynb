{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b27fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time, random\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2fe078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df(path): # takes in file path of df\n",
    "    try: \n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        if 'overdrive_author' not in list(df.columns):\n",
    "            df['overdrive_author'] = np.nan\n",
    "        start_index = df[df.overdrive_author.isna()].iloc[0].name\n",
    "#         print(start_index)\n",
    "    except FileNotFoundError:\n",
    "        return 0, 0\n",
    "    \n",
    "    return int(start_index), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a3d5ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df_redo(path): # takes in file path of df\n",
    "    try: \n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        if 'overdrive_author' not in list(df.columns):\n",
    "            df['overdrive_author'] = np.nan\n",
    "        sub_df = df[(df['Library copies'] == 'info unavailable') | (df.overdrive_author.isna())]\n",
    "        indices = list(sub_df.index)\n",
    "#         print(start_index)\n",
    "    except FileNotFoundError:\n",
    "        return 0, 0\n",
    "    \n",
    "    return indices, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "67b9cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_OD(path):\n",
    "    \n",
    "    global driver_proxy\n",
    "    driver_proxy = initiate_wbw()\n",
    "    \n",
    "    start_index, df = import_df(path)\n",
    "    base_url = 'https://nypl.overdrive.com'\n",
    "    \n",
    "\n",
    "\n",
    "    position_start_index = list(df.index).index(start_index)\n",
    "    list_of_indices = list(df.index[position_start_index:])\n",
    "    \n",
    "    \n",
    "    for index in list_of_indices:\n",
    "        \n",
    "        \n",
    "        if df.loc[index, 'overdrive_url'] != 'title not found':\n",
    "#         print('ding')\n",
    "            \n",
    "#             print (base_url + df.loc[index, 'overdrive_url'])\n",
    "#             break\n",
    "    \n",
    "    \n",
    "            soup = get_soup_driver_proxy(base_url + df.loc[index, 'overdrive_url'])\n",
    "        \n",
    "#             try: cookie_button = driver_proxy.find_elements_by_xpath('//button[@aria-label=\"Holds ratio explanation.\"]')\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "#             if df.loc[index, 'overdrive_author'] == '-1':\n",
    "            try: \n",
    "                overdrive_author = soup.find('a', class_='TitleDetailsHeading-creatorLink').text\n",
    "                df.loc[index, 'overdrive_author'] = overdrive_author\n",
    "            except Exception as e:\n",
    "                overdrive_author = -1\n",
    "                print(e)\n",
    "\n",
    "\n",
    "\n",
    "#             df.loc[index, 'overdrive_author'] = overdrive_author      \n",
    "\n",
    "            if df.loc[index, 'People waiting per copy'] == 'info unavailable':\n",
    "                \n",
    "                            # check for a holds_info button. If there, click it.\n",
    "                try:\n",
    "\n",
    "                    # click hold_time_info button\n",
    "                    buttons = driver_proxy.find_elements_by_xpath('//button[@aria-label=\"Holds ratio explanation.\"]')\n",
    "                    buttons[0].click()\n",
    "                    time.sleep(3 + random.uniform(-1,1))\n",
    "\n",
    "                    # grab html and parse into soup\n",
    "                    soup = bs(driver_proxy.page_source, 'html.parser') \n",
    "\n",
    "                    # extract hold_info like wait-time, number of people waiting, etc.\n",
    "                    holds_info_soup = soup.find_all('p', class_='detail')\n",
    "                    holds_info = [info.text.split(':') for info in holds_info_soup]\n",
    "\n",
    "                    # remove extra spaces\n",
    "                    list_results = []\n",
    "                    for row in holds_info:\n",
    "                        row[:] = [info.strip() for info in row]\n",
    "\n",
    "                    # convert result into sub_df\n",
    "                    holds_info = np.array(holds_info)\n",
    "                    sub_df = pd.DataFrame(holds_info.T, columns = holds_info[:,0]) ### sub_df might only be partially formed         \n",
    "                    sub_df.drop([0], inplace=True)\n",
    "\n",
    "                    # if holds_info contains incomplete holds_info, raise Error to remove data point\n",
    "    #                 print(sub_df.columns)\n",
    "    #                 print(list(sub_df.columns) != ['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'])\n",
    "                    if list(sub_df.columns) != ['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy']:\n",
    "                        raise Exception('column names don\\'t match')\n",
    "\n",
    "    #                 print('checked for holds_info button')\n",
    "    #                 display(sub_df)\n",
    "\n",
    "\n",
    "                # if no holds_info button, it may be because copies are available. Check.\n",
    "                except:\n",
    "    #                 print('apparently there was an error checking for holds_button button. maybe missing col names or no holds button')\n",
    "                    # grab html and parse into soup\n",
    "                    soup = bs(driver_proxy.page_source, 'html.parser')\n",
    "\n",
    "                    # yes copies are available and therefore no holds_info button\n",
    "                    try:\n",
    "                        # there is no holds_info button so check for borrow button\n",
    "                        if soup.find('button', class_='TitleActionButton').text == 'Borrow':\n",
    "                            copies_avail = soup.find('button', class_='TitleActionButton').parent.parent.parent.parent.find(class_='availabilityText').text.strip().split()\n",
    "                            if copies_avail[0] == 'Always':\n",
    "                                holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                         [0, 'Always available', 'Unknown', 0]])    \n",
    "                            else:    \n",
    "                                holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                         [0, copies_avail[2], str(-int(copies_avail[0])), 0]])\n",
    "    #                         print('there is a borrow button')\n",
    "                            sub_df = pd.DataFrame(holds_info, columns = holds_info[0]) \n",
    "                            sub_df.drop([0], inplace=True)\n",
    "\n",
    "                        else:\n",
    "    #                         print('there is no borrow button')\n",
    "                            raise Exception('no borrow button')\n",
    "\n",
    "                    # no copies available and no holds_info button\n",
    "                    except:\n",
    "    #                     print('no copies available and no holds_info button')\n",
    "                        holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                     ['info unavailable', 'info unavailable', 'info unavailable', 'info unavailable']])\n",
    "\n",
    "                        sub_df = pd.DataFrame(holds_info, columns = holds_info[0]) \n",
    "                        sub_df.drop([0], inplace=True)\n",
    "\n",
    "#                 display(sub_df)\n",
    "                df.loc[index, 'Wait time'] = sub_df.loc[1, 'Wait time']\n",
    "                df.loc[index, 'Library copies'] = sub_df.loc[1, 'Library copies']\n",
    "                df.loc[index, 'People waiting in total'] = sub_df.loc[1, 'People waiting in total']\n",
    "                df.loc[index, 'People waiting per copy'] = sub_df.loc[1, 'People waiting per copy']\n",
    "        elif df.loc[index, 'People waiting per copy'] == 'title not found':\n",
    "            df.loc[index, 'overdrive_author'] = -1\n",
    "        else:\n",
    "            df.loc[index, 'overdrive_author'] = 'unknown error in get_author_OD'\n",
    "        df.to_csv(path)\n",
    "        time.sleep(10 + random.uniform(-1,1))\n",
    "        \n",
    "#     return pd.DataFrame(list_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6105b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_OD_redo(path):\n",
    "    \n",
    "    global driver_proxy\n",
    "    driver_proxy = initiate_wbw()\n",
    "    \n",
    "    list_of_indices, df = import_df_redo(path)\n",
    "    base_url = 'https://nypl.overdrive.com'\n",
    "    \n",
    "\n",
    "\n",
    "#     position_start_index = list(df.index).index(start_index)\n",
    "#     list_of_indices = list(df.index[position_start_index:])\n",
    "    \n",
    "    \n",
    "    for index in list_of_indices:\n",
    "        \n",
    "        index = int(index)\n",
    "        if df.loc[index, 'overdrive_url'] != 'title not found':\n",
    "#         print('ding')\n",
    "            \n",
    "#             print (base_url + df.loc[index, 'overdrive_url'])\n",
    "#             break\n",
    "    \n",
    "    \n",
    "            soup = get_soup_driver_proxy(base_url + df.loc[index, 'overdrive_url'])\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "            if df.loc[index, 'overdrive_author'] == '-1':\n",
    "                try: overdrive_author = soup.find('a', class_='TitleDetailsHeading-creatorLink').text\n",
    "                except: overdrive_author = -1\n",
    "            \n",
    "\n",
    "\n",
    "#             df.loc[index, 'overdrive_author'] = overdrive_author      \n",
    "\n",
    "            if df.loc[index, 'People waiting per copy'] == 'info unavailable':\n",
    "                \n",
    "                            # check for a holds_info button. If there, click it.\n",
    "                \n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # click hold_time_info button\n",
    "                    buttons = driver_proxy.find_elements_by_xpath('//button[@aria-label=\"Holds ratio explanation.\"]')\n",
    "                    buttons[0].click()\n",
    "                    time.sleep(3 + random.uniform(-1,1))\n",
    "                    \n",
    "                    try: \n",
    "                        cookie_button = driver_proxy.find_elements_by_xpath('//button[@class=\"confirm button radius margin-small secondary right\"]')\n",
    "                        cookie_button.click()\n",
    "                        # let page load\n",
    "#                         time.sleep(2 + random.uniform(-1,0))\n",
    "                        buttons = driver_proxy.find_elements_by_xpath('//button[@aria-label=\"Holds ratio explanation.\"]')\n",
    "                        buttons[0].click()\n",
    "#                         time.sleep(3 + random.uniform(-1,1))\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "                    # grab html and parse into soup\n",
    "                    soup = bs(driver_proxy.page_source, 'html.parser') \n",
    "\n",
    "                    # extract hold_info like wait-time, number of people waiting, etc.\n",
    "                    holds_info_soup = soup.find_all('p', class_='detail')\n",
    "                    holds_info = [info.text.split(':') for info in holds_info_soup]\n",
    "\n",
    "                    # remove extra spaces\n",
    "                    list_results = []\n",
    "                    for row in holds_info:\n",
    "                        row[:] = [info.strip() for info in row]\n",
    "\n",
    "                    # convert result into sub_df\n",
    "                    holds_info = np.array(holds_info)\n",
    "                    sub_df = pd.DataFrame(holds_info.T, columns = holds_info[:,0]) ### sub_df might only be partially formed         \n",
    "                    sub_df.drop([0], inplace=True)\n",
    "\n",
    "                    # if holds_info contains incomplete holds_info, raise Error to remove data point\n",
    "    #                 print(sub_df.columns)\n",
    "    #                 print(list(sub_df.columns) != ['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'])\n",
    "                    if list(sub_df.columns) != ['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy']:\n",
    "                        raise Exception('column names don\\'t match')\n",
    "                     \n",
    "    #                 print('checked for holds_info button')\n",
    "#                         #                 display(sub_df)\n",
    "#                         try:  \n",
    "#                             df.loc[index, 'Wait time'] = sub_df.loc[1, 'Wait time']\n",
    "#                         except:\n",
    "#                             pass                        \n",
    "#                         try:\n",
    "#                             df.loc[index, 'Library copies'] = sub_df.loc[1, 'Library copies']\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         try:  \n",
    "#                             df.loc[index, 'People waiting in total'] = sub_df.loc[1, 'People waiting in total']\n",
    "#                         except:\n",
    "#                             try:\n",
    "#                                 df.loc[index, 'People waiting in total'] = sub_df.loc[1, 'People waiting']\n",
    "#                             except:\n",
    "#                                 pass\n",
    "#                         try:  \n",
    "#                             df.loc[index, 'People waiting per copy'] = sub_df.loc[1, 'People waiting per copy']\n",
    "#                         except:\n",
    "#                             pass\n",
    "\n",
    "                # if no holds_info button, it may be because copies are available. Check.\n",
    "                except:\n",
    "    #                 print('apparently there was an error checking for holds_button button. maybe missing col names or no holds button')\n",
    "                    # grab html and parse into soup\n",
    "                    soup = bs(driver_proxy.page_source, 'html.parser')\n",
    "\n",
    "                    # yes copies are available and therefore no holds_info button\n",
    "                    try:\n",
    "                        # there is no holds_info button so check for borrow button\n",
    "                        if soup.find('button', class_='TitleActionButton').text == 'Borrow':\n",
    "                            copies_avail = soup.find('button', class_='TitleActionButton').parent.parent.parent.parent.find(class_='availabilityText').text.strip().split()\n",
    "                            if copies_avail[0] == 'Always':\n",
    "                                holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                         [0, 'Always available', 'Unknown', 0]])    \n",
    "                            else:    \n",
    "                                holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                         [0, copies_avail[2], str(-int(copies_avail[0])), 0]])\n",
    "    #                         print('there is a borrow button')\n",
    "                            sub_df = pd.DataFrame(holds_info, columns = holds_info[0]) \n",
    "                            sub_df.drop([0], inplace=True)\n",
    "\n",
    "                        else:\n",
    "    #                         print('there is no borrow button')\n",
    "                            raise Exception('no borrow button')\n",
    "\n",
    "                    # no copies available and no holds_info button\n",
    "                    except:\n",
    "    #                     print('no copies available and no holds_info button')\n",
    "                        holds_info = np.array([['Wait time', 'Library copies', 'People waiting in total', 'People waiting per copy'],\n",
    "                                     ['info unavailable', 'info unavailable', 'info unavailable', 'info unavailable']])\n",
    "\n",
    "                        sub_df = pd.DataFrame(holds_info, columns = holds_info[0]) \n",
    "                        sub_df.drop([0], inplace=True)\n",
    "\n",
    "#                 display(sub_df)\n",
    "                df.loc[index, 'Wait time'] = sub_df.loc[1, 'Wait time']\n",
    "                df.loc[index, 'Library copies'] = sub_df.loc[1, 'Library copies']\n",
    "                df.loc[index, 'People waiting in total'] = sub_df.loc[1, 'People waiting in total']\n",
    "                df.loc[index, 'People waiting per copy'] = sub_df.loc[1, 'People waiting per copy']\n",
    "        elif df.loc[index, 'People waiting per copy'] == 'title not found':\n",
    "            df.loc[index, 'overdrive_author'] = -1\n",
    "        else:\n",
    "            df.loc[index, 'overdrive_author'] = 'unknown error in get_author_OD'\n",
    "        df.to_csv(path)\n",
    "        time.sleep(10 + random.uniform(-1,1))\n",
    "        \n",
    "#     return pd.DataFrame(list_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfd5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_wbw(): # launches headless browser selenium\n",
    "    options = Options()\n",
    "    options.headless = False\n",
    "    return webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6d235f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_driver_proxy(base_url): # Retrieves soup from website using Selenium\n",
    "#     driver = webdriver.Chrome()\n",
    "    YOUR_API_KEY = 'OavaC96DqSNsO1vM2qStOhzTZoeOakNd'\n",
    "    target_url = urllib.parse.quote(base_url, safe='~')\n",
    "\n",
    "    URL = f\"https://api.webscrapingapi.com/v1/?api_key={YOUR_API_KEY}&url=\" + target_url\n",
    "\n",
    "    driver_proxy.get(URL)\n",
    "    time.sleep(2 + random.uniform(-1,1))\n",
    "\n",
    "    # check if accept cookie button\n",
    "\n",
    "    try: \n",
    "        cookie_button = driver_proxy.find_elements_by_xpath('//button[@class=\"confirm button radius margin-small secondary right\"]')\n",
    "        cookie_button[0].click()\n",
    "        time.sleep(3 + random.uniform(-1,0))\n",
    "#         driver_proxy.get(URL)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    r = driver_proxy.page_source\n",
    "    \n",
    "#     driver.quit()\n",
    "    return bs(r, 'html.parser')  \n",
    "#     return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6be775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global driver_proxy\n",
    "driver_proxy = initiate_wbw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "507c6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_proxy.get('https://nypl.overdrive.com/media/599788')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bf506e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup_driver_proxy('https://nypl.overdrive.com/media/3783541')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6da5870",
   "metadata": {},
   "source": [
    "authors = soup.find_all('a', class_='TitleDetailsHeading-creatorLink')\n",
    "for author in authors:\n",
    "    print (author.text.strip())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "875402f5",
   "metadata": {},
   "source": [
    "get_author_OD('Figures/get_author_OD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1580a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_proxy.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4ac437cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(path):\n",
    "    error_counter = 0\n",
    "    try: get_author_OD(path)\n",
    "    except:\n",
    "        print('error')\n",
    "        time.sleep(60 + random.uniform(-2,2))\n",
    "        error_counter += 1\n",
    "        if error_counter > 3:\n",
    "            return 'Terminated, check for errors'\n",
    "        scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa10b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scrape_redo(path, error_counter=0):\n",
    "    error_counter = error_counter\n",
    "    try: get_author_OD_redo(path)\n",
    "    except:\n",
    "        print('error')\n",
    "        time.sleep(60 + random.uniform(-2,2))\n",
    "        error_counter += 1\n",
    "        if error_counter > 2:\n",
    "            return 'Terminated, check for errors'\n",
    "        new_scrape_redo(path, error_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ff884ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape('Figures/get_author_OD.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51319f2f",
   "metadata": {},
   "source": [
    "global start_time, error_counter\n",
    "start_time = time.time()\n",
    "error_counter = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceffeb01",
   "metadata": {},
   "source": [
    "def scrape(path, current_time):\n",
    "    elapsed_time = current_time - time.time()\n",
    "    current_time = time.time()\n",
    "    if elapsed_time < 5:\n",
    "        return 'Done'\n",
    "    try:test_func()\n",
    "    #     try: get_author_OD(path)\n",
    "    except:\n",
    "        print('error')\n",
    "        error_counter += 1\n",
    "        time.sleep(60 + random.uniform(-2,2))\n",
    "        if error_counter > 3:\n",
    "            return 'terminated with error'\n",
    "        scrape(path, time)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2057281c",
   "metadata": {},
   "source": [
    "def test_func():\n",
    "    time.sleep(6)\n",
    "    test_func()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14166dd6",
   "metadata": {},
   "source": [
    "scrape('sfd', 1641680145.82071)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5109e0fc",
   "metadata": {},
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830db863",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \n",
    "test_df = pd.read_csv(path, index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248da0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
