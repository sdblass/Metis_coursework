{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d9a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_funcs import *\n",
    "import string, re, emoji\n",
    "from langdetect import detect\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65c6a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, import text_df pickle if restarting kernel. Langdetect takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0af32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>comments_or_subs</th>\n",
       "      <th>democracy_score</th>\n",
       "      <th>GDP</th>\n",
       "      <th>abstained</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>india</td>\n",
       "      <td>comments</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2946061.0</td>\n",
       "      <td>True</td>\n",
       "      <td>&amp;gt; Indian students walk out of Kharkiv, as b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>india</td>\n",
       "      <td>comments</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2946061.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The Kremlin readout of the Modi-Putin telephon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>india</td>\n",
       "      <td>comments</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2946061.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia is part of the UNGA, everyone in the bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>india</td>\n",
       "      <td>comments</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2946061.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Hopefully one thing this tradgedy has taught u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>india</td>\n",
       "      <td>comments</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2946061.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Look at a map of India and its surrounding cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country comments_or_subs  democracy_score        GDP abstained  \\\n",
       "0   india         comments             6.91  2946061.0      True   \n",
       "1   india         comments             6.91  2946061.0      True   \n",
       "2   india         comments             6.91  2946061.0      True   \n",
       "3   india         comments             6.91  2946061.0      True   \n",
       "4   india         comments             6.91  2946061.0      True   \n",
       "\n",
       "                                                text  \n",
       "0  &gt; Indian students walk out of Kharkiv, as b...  \n",
       "1  The Kremlin readout of the Modi-Putin telephon...  \n",
       "2  Russia is part of the UNGA, everyone in the bo...  \n",
       "3  Hopefully one thing this tradgedy has taught u...  \n",
       "4  Look at a map of India and its surrounding cou...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_pickle('data/text_df')\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc47be5",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "## Remove URLs\n",
    "These do not add anything to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4793cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2205b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c88885",
   "metadata": {},
   "source": [
    "## Detect language\n",
    "This will be important for things like spell check and emoji processing to know what language a post is written in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756956b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        text = text[:100] # truncate text to speed up algorithm\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ae9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this will take some time to run\n",
    "text_df['language'] = text_df.text.apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f34d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_pickle('data/text_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2793c97",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "These interfere with the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39fbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return re.sub('[%s]' % re.escape(string.punctuation), ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "471d0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6970be0",
   "metadata": {},
   "source": [
    "## Replace emojis with text\n",
    "I will replace emojis with English or whatever the language of the post is to make text analysis easier. I will follow the instructions from [here](https://stackoverflow.com/questions/57580288/how-to-replace-emoji-to-word-in-a-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35b7a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emojis(text, language):\n",
    "    if len(language) > 2:\n",
    "        language = 'en'\n",
    "        \n",
    "    # remove emojis and set delimiters to \"\" because otherwise it will put ':translation:' instead of 'translation'\n",
    "    text = emoji.demojize(text, language = language, delimiters=(\"\", \"\")) \n",
    "    text = text.replace('_', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99169118",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.apply(lambda x: clean_emojis(x.text, x.language), axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "871bd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_pickle('data/text_df_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f6533",
   "metadata": {},
   "source": [
    "## Convert all text to lower case\n",
    "Will make vectorizing easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "691f0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77deefef",
   "metadata": {},
   "source": [
    "## Spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59f460bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text, language):\n",
    "    if language not in 'en, pl, ru, uk, tr, es, pt, cs, el, it, fr, vi'.split(', '): # these are supported langs\n",
    "        return text\n",
    "    spell = Speller(language)\n",
    "    return spell(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09af2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "CPU times: user 10h 5min 52s, sys: 2min 56s, total: 10h 8min 48s\n",
      "Wall time: 10h 8min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter=0\n",
    "# 257,562 rows takes 10 hours to run\n",
    "for row in range(len(text_df)):\n",
    "    language = text_df.loc[row, 'language']\n",
    "    if language in 'en, pl, ru, uk, tr, es, pt, cs, el, it, fr, vi'.split(', '): # these are supported langs\n",
    "        text_df.loc[row, 'text'] = spell_check(text_df.loc[row, 'text'], language)\n",
    "    counter += 1\n",
    "    if counter%1000==0: print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1e63cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_pickle('data/text_df_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281422c",
   "metadata": {},
   "source": [
    "## Remove numbers\n",
    "Numbers do not contribute to the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ceff862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'[0-9]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cd1f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "06757778",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_pickle('data/text_df_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b722790",
   "metadata": {},
   "source": [
    "Now moving on further pre-processing by removing stop words and lemmatizing in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c1f1f",
   "metadata": {},
   "source": [
    "## Spell check only\n",
    "All that cleaning could impact the sentiment scores unnecessarily so we will redo the spell check, then translate to English, then run sentiment analysis on the results. Before spell check, we will remove hyperlinks, numbers, and detect language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba9c49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_pickle('data/text_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70e7e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b711b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.text = text_df.text.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcb89bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "CPU times: user 13h 15min 57s, sys: 4min 51s, total: 13h 20min 48s\n",
      "Wall time: 13h 57min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter=0\n",
    "# 257,562 rows takes 14 hours to run with less cleaning\n",
    "for row in range(len(text_df)):\n",
    "    language = text_df.loc[row, 'language']\n",
    "    if language in 'en, pl, ru, uk, tr, es, pt, cs, el, it, fr, vi'.split(', '): # these are supported languages\n",
    "        text_df.loc[row, 'text'] = spell_check(text_df.loc[row, 'text'], language)\n",
    "    counter += 1\n",
    "    if counter%10000==0: \n",
    "        print(counter)\n",
    "        text_df.to_pickle('data/text_df_spelling')\n",
    "        \n",
    "text_df.to_pickle('data/text_df_spelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eda066",
   "metadata": {},
   "source": [
    "Before running translation, we will sentence-tokenize and translate each sentence. That will be followed by sentiment analysis in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0fd7f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_spelling_sent_tokenized = pd.DataFrame(columns = text_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ea925018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the columns that will stay the same for each sentence in a post\n",
    "cols = ['country', 'comments_or_subs', 'democracy_score', 'GDP', 'abstained', 'language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e0ca35cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "CPU times: user 8h 5min 51s, sys: 2min 2s, total: 8h 7min 54s\n",
      "Wall time: 8h 10min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = 0\n",
    "\n",
    "# This will 8 hours to run\n",
    "\n",
    "for row in range(len(text_df)):\n",
    "    sentences = sent_tokenize(text_df.loc[row, 'text'])\n",
    "    for sentence in sentences:\n",
    "        sub_df = pd.DataFrame(text_df.loc[row, cols]).transpose()#, columns = text_df.columns)\n",
    "        sub_df.loc[row, 'text'] = sentence\n",
    "        text_df_spelling_sent_tokenized = pd.concat([text_df_spelling_sent_tokenized, sub_df])\n",
    "\n",
    "    counter += 1\n",
    "    if counter%1000==0:\n",
    "        print(counter)\n",
    "        text_df_spelling_sent_tokenized.to_pickle('data/text_df_spelling_sent_tokenized')\n",
    "#         break\n",
    "text_df_spelling_sent_tokenized.to_pickle('data/text_df_spelling_sent_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "409ec1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940857, 7)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_spelling_sent_tokenized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d6488",
   "metadata": {},
   "source": [
    "Now that sentence tokenizing is done, we will get the VADER sentiment analysis for each sentence in the next notebook. After that, we will proceed to text analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
