{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d64328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from gensim import corpora, models, matutils\n",
    "import logging\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from corex_funcs import *\n",
    "from translate import Translator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f73c26",
   "metadata": {},
   "source": [
    "# Foreign language analysis\n",
    "In this final notebook, we will look to see if there are differences in sentiment between posters in English and those in foreign languages. We will restrict topic analysis only to those languages that SpaCy supports so we can properly lemmatize words and identify parts of speech and adjective modifiers. Because translation packages often require a paid subscription or have rate limits, we will do topic analysis first, then translate the topics into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668d60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('data/df_parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a180d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parsed_non_en = df.loc[df.language != 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128298e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parsed_non_en.to_pickle('data/df_parsed_non_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca93dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_non_en = pd.read_pickle('data/df_parsed_non_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857025c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence): # take in spacy token, return lemmas and remove numbers, punctuation, and stop words\n",
    "    return ' '.join([token.lemma_ for token in sentence if token.pos_ != 'NUM' and token.pos_ != 'PUNCT' and not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3824c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_non_en['spacy_doc_cleaned'] = df_parsed_non_en.spacy_doc.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df690c43",
   "metadata": {},
   "source": [
    "What are the different non-English languages in the df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00af4320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it    41726\n",
       "fr    34001\n",
       "de     1603\n",
       "da      527\n",
       "zh      320\n",
       "lt      173\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parsed_non_en.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7977512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict to translate language code to subreddit name\n",
    "language_dict = {k:v for k, v in zip(df_parsed_non_en.language.value_counts().index, 'italy, france, catalan, germany, denmark, china, lithuania, netherlands'.split(', '))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1e638",
   "metadata": {},
   "source": [
    "For topic modeling, we'll use CorEx because that had the best results with the English posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "775f7c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['it', 'fr', 'de', 'da', 'zh', 'lt'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parsed_non_en.language.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c321980",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for language in df_parsed_non_en.language.value_counts().index:\n",
    "    sub_df = df_parsed_non_en[df_parsed_non_en.language == language].copy()\n",
    "    results[language] = corex_translate(language, sub_df, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d36587bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_lt: pysanky, kirashchuk, wax, using, member, egg, folk, oleh, pysanka, putins\n",
      "0_en: pysanky, kirashchuk, wax, using, member, egg, Folk, oleh, pysanka, putins\n",
      "\n",
      "\n",
      "1_lt: slava, ukraine, ukraini, ukrainian, putin, charities, cams, live, linkti, vladimir\n",
      "1_en: slava, ukraine, ukraini, ukrainian, putin, charities, cams, Live, nod, vladimir\n",
      "\n",
      "\n",
      "2_lt: amp, xb, kyiv, lviv, supplies, village, unique, kiev, today, nato\n",
      "2_en: (amf) -, Xb, kyiv, lviv, supplies, Village”, Unique, Kiev, today., NATO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for language in ['lt']:\n",
    "    sub_df = df_parsed_non_en[df_parsed_non_en.language == language].copy()\n",
    "    results[language] = corex_translate(language, sub_df, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a07ba",
   "metadata": {},
   "source": [
    "Cleaning seems to have deleted all Chinese words so we will redo Corex on Chinese and Lithuanian (the remaining languages) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2fcafaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_parsed_non_en.loc[df_parsed_non_en.language=='zh', 'spacy_doc_cleaned'] = df_parsed_non_en.loc[df_parsed_non_en.language=='zh', 'spacy_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f76ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_non_en.loc[df_parsed_non_en.language == 'zh', 'spacy_doc_cleaned'] = df_parsed_non_en.loc[df_parsed_non_en.language == 'zh', 'spacy_doc'].map(lambda x: ''.join(list(x.text))).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a51eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_zh: aggression, taiwan, government, pro, does, army, ukrainian, war, chinese, attack\n",
      "0_en: The exceptional resort to self-defence is contingent on the occurrence of an  \"armed attack \", which is rendered in French as  \"agression armée \", i.e., armed aggression., Malaysia, Government, pro, `Who does what&apos;, Islamic Army of Aden;, Ukrainian, Go to war, LocaleName, Unmanned attack vehicles\n",
      "\n",
      "\n",
      "1_zh: make, foreign, china, mistakes, states, united, ukraine, media, feel, position\n",
      "1_en: Make, foreign operation, We need to build a wall., It doesn't matter if you make mistakes., STATES, United, Ukraine, media, feel, Position\n",
      "\n",
      "\n",
      "2_zh: strength, evidence, military, american, bad, really, attitude, germany, security, far\n",
      "2_en: Strength:, <g id=\"1\">evidence</g><g id=\"2\"> integration</g>, Military, American Tower, BAD, Really?, Attitude Demo, Germany, Security, Far Easern Technical University\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "language = 'zh'\n",
    "sub_df = df_parsed_non_en[df_parsed_non_en.language == language].copy()\n",
    "results[language] = corex_translate(language, sub_df, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c091324",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated anchors from en to it: [['presidente', 'putin'], ['nato'], ['polonia']]\n",
      "0: putin, presidente, oligarca, vladimir, pro, salvini, macron, pazzo, maglietta, criminale\n",
      "0_en: Putin, President, oligarch, Vladimir, pro, Salvini, MACRON, Crazy, t-shirt, Criminal\n",
      "\n",
      "\n",
      "1: nato, entrare, ue, membro, espansione, difensivo, intervenire, attaccare, est, alleanza\n",
      "1_en: born in, I enter, EU, ember, expansion, defensive, I intervene, to attach, basement floor plan, SAVINGS PLANS\n",
      "\n",
      "\n",
      "2: il, polonia, russo, dell, ucraina, essere, russia, paese, potere, guerra\n",
      "2_en: on ,, POLAND, Russian, for, Ukraine, have, russia, country, power, war\n",
      "\n",
      "\n",
      "Percent Topic 0 Documents: 14.49%\n",
      "Percent Topic 1 Documents: 8.72%\n",
      "Percent Topic 2 Documents: 32.6%\n",
      "Translated anchors from en to fr: [['• présidente\\xa0:', 'huilo'], ['otan'], ['polonia']]\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: • présidente :\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: huilo\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: polonia\n",
      "0: otan, rejoindre, ue, membre, extension, alliance, expansion, unien, rentrer, caritatif\n",
      "0_en: NATO, join, &#13;, Number, extension, covenant, Expansion, unien, ship, Charity\n",
      "\n",
      "\n",
      "1: et, gt, amp, xb, jean, mars, article, luc, france, international\n",
      "1_en: and, LAUNCH, amp, xb, john, March, Articles, luke, France, internationale\n",
      "\n",
      "\n",
      "2: russe, russie, ukraine, ukrainien, aujourd, hui, contre, militaire, guerre, pays\n",
      "2_en: Russian, Russia, Ukraine, ukrainisch, Today, Hui, against, militaire, war, country\n",
      "\n",
      "\n",
      "Percent Topic 0 Documents: 4.21%\n",
      "Percent Topic 1 Documents: 18.42%\n",
      "Percent Topic 2 Documents: 32.08%\n",
      "Translated anchors from en to de: [['(directeur)', 'huilo'], ['otan'], ['somalia 0 0 0 0 0 0 0 0 0']]\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: (directeur)\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: huilo\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: otan\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: somalia 0 0 0 0 0 0 0 0 0\n",
      "0: ukraine, putin, fuck, region, liveuamap, hr, kiev, stand, et, la\n",
      "0_en: Ukraine, Putin, test, region, liveuamap, hr, - Kiev, stood, et, la\n",
      "\n",
      "\n",
      "1: russian, frau, forces, kyiv, gas, russians, entscheiden, internationale, breaking, news\n",
      "1_en: Russian, Ms., Baghdad., (Fribourg), gas, russians, do, International, Breaking, news\n",
      "\n",
      "\n",
      "2: deutschland, gut, nato, regierung, weapons, mariupol, zeigen, russland, country, anti\n",
      "2_en: Germany, good, nato, Government, WEAPONS, Mariupol, to show, Russia, country, anti\n",
      "\n",
      "\n",
      "Percent Topic 0 Documents: 36.49%\n",
      "Percent Topic 1 Documents: 14.22%\n",
      "Percent Topic 2 Documents: 85.53%\n",
      "Translated anchors from en to da: [['direktør', 'huilo'], ['otan'], ['somalia 0 0 0 0 0 0 0 0 0 0 0']]\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: direktør\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: huilo\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: otan\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: somalia 0 0 0 0 0 0 0 0 0 0 0\n",
      "0: deleted, kiev, kyiv, gt, going, like, ve, leaders, killed, putin\n",
      "0_en: deleted, Kiev, kyiv, &gt;, Going, Like, vc, Leaders, Killed, Vladimir Putin\n",
      "\n",
      "\n",
      "1: ukraine, feel, video, think, comes, isn, says, said, free, war\n",
      "1_en: Switzerland, FEEL, video, Think Global, Comes, ISN, Says, said, free, Warhammer Online: Age of Reckoning\n",
      "\n",
      "\n",
      "2: zelensky, volodymyr, president, nato, elon, russian, musk, strikes, starlink, fucked\n",
      "2_en: zelensky, volodymyr, The President, NATO, Elon, Russian, Musk, Strikes, starlink, fucked\n",
      "\n",
      "\n",
      "Percent Topic 0 Documents: 80.46%\n",
      "Percent Topic 1 Documents: 90.51%\n",
      "Percent Topic 2 Documents: 89.56%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/metis2/lib/python3.8/site-packages/translate/translate.py:45\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m text_list \u001b[38;5;241m=\u001b[39m wrap(text, TRANSLATION_API_MAX_LENGHT, replace_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_wraped\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text_wraped \u001b[38;5;129;01min\u001b[39;00m text_list)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/metis2/lib/python3.8/site-packages/translate/providers/mymemory_translated.py:49\u001b[0m, in \u001b[0;36mMyMemoryProvider.get_translation\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     48\u001b[0m matches \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m next_best_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_best_match[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m df_parsed_non_en\u001b[38;5;241m.\u001b[39mlanguage\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m      4\u001b[0m     sub_df \u001b[38;5;241m=\u001b[39m df_parsed_non_en[df_parsed_non_en\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m language]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 5\u001b[0m     results[language] \u001b[38;5;241m=\u001b[39m \u001b[43mcorex_anchors_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36mcorex_anchors_translate\u001b[0;34m(language, anchors, data, anchor_strength, num_topics)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(anchors):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[0;32m---> 10\u001b[0m         anchors[row_index][col_index] \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslated anchors from en to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m,\n\u001b[1;32m     15\u001b[0m                          stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                          token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mb[a-z][a-z]+\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                          binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/metis2/lib/python3.8/site-packages/translate/translate.py:45\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m     44\u001b[0m text_list \u001b[38;5;241m=\u001b[39m wrap(text, TRANSLATION_API_MAX_LENGHT, replace_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_wraped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_wraped\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "anchors = [['president', 'putin'],['nato'], ['poland']]\n",
    "for language in df_parsed_non_en.language.value_counts().index:\n",
    "    sub_df = df_parsed_non_en[df_parsed_non_en.language == language].copy()\n",
    "    results[language] = corex_anchors_translate(language, anchors, sub_df, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3ad6d",
   "metadata": {},
   "source": [
    "The free translator API limit is very low so it is not feasible to do much translation work for this project without buying a license so we'll stop here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis2]",
   "language": "python",
   "name": "conda-env-metis2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
